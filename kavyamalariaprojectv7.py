# -*- coding: utf-8 -*-
"""kavyamalariaprojectv7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/kavyas2003/COSMOSColab/blob/main/kavyamalariaprojectv7.ipynb
"""

import numpy as np
import matplotlib.pyplot as plt

import keras
#import the layers used to build the neural network layers
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
#connect to drive to access data

from keras.preprocessing.image import ImageDataGenerator
#preprocess the images for data


train_datagen = ImageDataGenerator(rescale = 1./255,
shear_range = 0.2,
zoom_range = 0.2,
horizontal_flip = True)
test_datagen = ImageDataGenerator(rescale = 1./255)

train_generator = train_datagen.flow_from_directory(
        '/content/drive/My Drive/train',  # this is the target directory
        target_size=(150, 150),  # all images will be resized to 150x150
        batch_size=16,
        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels

# this is a similar generator, for validation data
validation_generator = test_datagen.flow_from_directory(
        '/content/drive/My Drive/train',
        target_size=(150, 150),
        batch_size=16,
        class_mode='binary')

test_generator = test_datagen.flow_from_directory(
        '/content/drive/My Drive/test',
         target_size=(150,150),
         batch_size=16,
         class_mode='binary')

print(type(train_generator))
       
  #preprocess the images to normalize the data

Dense = keras.layers.Dense #build the neural network and create all the layers 
Sequential = keras.Sequential
classifier = Sequential()

classifier.add(Conv2D(32, (3, 3), input_shape = (150 , 150, 3), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2, 2)))
classifier.add(MaxPooling2D(pool_size = (2, 2)))
classifier.add(Flatten())
classifier.add(Dense(128, activation = 'relu'))
classifier.add(Dense(64, activation = 'relu'))
classifier.add(Dense(32, activation = 'relu'))
classifier.add(Dense(16, activation = 'relu'))
classifier.add(Dense(1, activation = 'sigmoid'))

classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) #train the model 
               # Train the model, iterating on the data in batches of 32 samples

history = classifier.fit_generator(train_generator,
          steps_per_epoch = 50,
          epochs = 1,
          validation_steps = 100, validation_data = validation_generator)
 #run the model on the validation data to test for accuracy

result = classifier.evaluate_generator(generator = validation_generator, steps=50)
print( "Accuracy is:", result[1])
print( "Loss is:", result[0])

result = classifier.evaluate_generator(generator = test_generator, steps=50)
print( "Accuracy is:", result[1])
print( "Loss is:", result[0])

#evaluate and print the loss and the accuracy

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Number of epochs')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
#see the model's history and performance on the validation data
#print the loss function
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()